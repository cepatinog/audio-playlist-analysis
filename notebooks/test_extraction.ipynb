{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth enabled on GPU.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "import os\n",
    "# Hide unnecessary TensorFlow messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import sys\n",
    "import essentia.standard as es\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Determine project root (assuming the notebook is in the notebooks/ folder)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Current PYTHONPATH (first few entries):\", sys.path[:3])\n",
    "\n",
    "# Define paths\n",
    "raw_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "sample_audio = os.path.join(raw_dir, \"example.mp3\")  # Ensure this file exists\n",
    "tempo_model_file = os.path.join(project_root, \"src\", \"deeptemp-k16-3.pb\")\n",
    "print(\"Sample audio file:\", sample_audio)\n",
    "print(\"Tempo model file:\", tempo_model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Audio Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_audio import load_audio_file\n",
    "\n",
    "audio_dict = load_audio_file(sample_audio, targetMonoSampleRate=44100, targetTempoSampleRate=11025)\n",
    "\n",
    "print(\"Returned keys:\")\n",
    "pprint(list(audio_dict.keys()))\n",
    "\n",
    "print(\"\\nDetails of loaded audio:\")\n",
    "print(\"Stereo audio (first 5 samples):\")\n",
    "pprint(audio_dict['stereo_audio'][:5])\n",
    "print(\"Mono audio length (for key extraction):\", len(audio_dict['mono_audio']))\n",
    "print(\"Mono audio length (for tempo extraction):\", len(audio_dict['mono_tempo']))\n",
    "print(\"Sample rate used for mono audio:\", audio_dict['sampleRate'])\n",
    "print(\"Number of channels in original file:\", audio_dict['numChannels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Individual Feature Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_tempo import extract_tempo_features\n",
    "from src.extract_key import extract_key_features\n",
    "from src.extract_loudness import extract_loudness_features\n",
    "\n",
    "tempo_features = extract_tempo_features(audio_dict['mono_tempo'], method='tempocnn', model_file=tempo_model_file)\n",
    "print(\"Tempo Features:\")\n",
    "pprint(tempo_features)\n",
    "\n",
    "key_features = extract_key_features(audio_dict['mono_audio'])\n",
    "print(\"\\nKey Features:\")\n",
    "pprint(key_features)\n",
    "\n",
    "loudness_features = extract_loudness_features(audio_dict['stereo_audio'], hopSize=1024/44100, sampleRate=44100, startAtZero=True)\n",
    "print(\"\\nLoudness Features:\")\n",
    "pprint(loudness_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import embedding extraction functions.\n",
    "from src.extract_embeddings import extract_discogs_effnet_embeddings, extract_msd_musicnn_embeddings\n",
    "\n",
    "# Load audio for embeddings: using MonoLoader at 16 kHz.\n",
    "audio_embeddings = es.MonoLoader(filename=sample_audio, sampleRate=16000, resampleQuality=4)()\n",
    "print(\"Loaded audio for embeddings length:\", len(audio_embeddings))\n",
    "\n",
    "# Define model paths.\n",
    "discogs_model_file = os.path.join(project_root, \"src\", \"discogs-effnet-bs64-1.pb\")  # Update filename if necessary\n",
    "musicnn_model_file = os.path.join(project_root, \"src\", \"msd-musicnn-1.pb\")           # Update filename if necessary\n",
    "\n",
    "# Extract Discogs-Effnet embeddings.\n",
    "discogs_embedding = extract_discogs_effnet_embeddings(audio_embeddings, model_file=discogs_model_file)\n",
    "print(\"Discogs-Effnet embedding shape:\", discogs_embedding.shape)\n",
    "print(\"Discogs-Effnet embedding:\")\n",
    "pprint(discogs_embedding)\n",
    "\n",
    "# Extract MSD-MusicCNN embeddings.\n",
    "musicnn_embedding = extract_msd_musicnn_embeddings(audio_embeddings, model_file=musicnn_model_file)\n",
    "print(\"MSD-MusicCNN embedding shape:\", musicnn_embedding.shape)\n",
    "print(\"MSD-MusicCNN embedding:\")\n",
    "pprint(musicnn_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Genre Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_genre import extract_genre_features\n",
    "\n",
    "# Define the path to the Genre Discogs400 model file.\n",
    "genre_model_file = os.path.join(project_root, \"src\", \"genre_discogs400-discogs-effnet-1.pb\")\n",
    "print(\"Genre model file:\", genre_model_file)\n",
    "\n",
    "# Use the previously extracted Discogs-Effnet embedding (discogs_embedding)\n",
    "# Make sure that discogs_embedding is a 1D numpy array (averaged over frames).\n",
    "genre_predictions = extract_genre_features(discogs_embedding, model_file=genre_model_file)\n",
    "\n",
    "print(\"Genre predictions shape:\", genre_predictions.shape)\n",
    "print(\"Genre predictions:\")\n",
    "pprint(genre_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test voice/instrumental classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_voice_instrumental import extract_voice_instrumental\n",
    "\n",
    "# Assuming discogs_embedding was already extracted (and is a 1D vector)\n",
    "# Ensure it's reshaped to 2D if needed:\n",
    "if discogs_embedding.ndim == 1:\n",
    "    discogs_embedding = np.expand_dims(discogs_embedding, axis=0)\n",
    "\n",
    "voice_result = extract_voice_instrumental(discogs_embedding, model_file=os.path.join(project_root, \"src\", \"voice_instrumental-discogs-effnet-1.pb\"))\n",
    "print(\"Voice/Instrumental Classification:\")\n",
    "pprint(voice_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test danceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_danceability import extract_danceability_features\n",
    "\n",
    "# Test signal-based danceability extraction:\n",
    "dance_signal = extract_danceability_features(audio_dict['mono_audio'], mode=\"signal\", sampleRate=44100)\n",
    "print(\"Signal-based Danceability:\")\n",
    "pprint(dance_signal)\n",
    "\n",
    "# Test classifier-based danceability extraction:\n",
    "# Ensure discogs_embedding is 2D:\n",
    "if discogs_embedding.ndim == 1:\n",
    "    discogs_embedding = np.expand_dims(discogs_embedding, axis=0)\n",
    "dance_classifier = extract_danceability_features(discogs_embedding, mode=\"classifier\", model_file=os.path.join(project_root, \"src\", \"danceability-discogs-effnet-1.pb\"))\n",
    "print(\"\\nClassifier-based Danceability:\")\n",
    "pprint(dance_classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_arousal_valence import extract_arousal_valence_features\n",
    "\n",
    "# Load audio for emotion extraction using MonoLoader at 16kHz.\n",
    "audio_emotion = es.MonoLoader(filename=sample_audio, sampleRate=16000, resampleQuality=4)()\n",
    "print(\"Loaded audio for emotion extraction length:\", len(audio_emotion))\n",
    "\n",
    "# Test arousal/valence extraction.\n",
    "# Note: We use MSD-MusicCNN embeddings for emotion extraction.\n",
    "emotion_predictions = extract_arousal_valence_features(audio_emotion,\n",
    "                                                       embedding_model_file=os.path.join(project_root, \"src\", \"msd-musicnn-1.pb\"),\n",
    "                                                       regression_model_file=os.path.join(project_root, \"src\", \"emomusic-msd-musicnn-2.pb\"))\n",
    "print(\"Arousal/Valence predictions:\")\n",
    "pprint(emotion_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Integrated Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.audio_analysis2 import extract_all_features\n",
    "\n",
    "# Define additional model paths for genre, voice/instrumental, danceability, and emotion.\n",
    "genre_model_file = os.path.join(project_root, \"src\", \"genre_discogs400-discogs-effnet-1.pb\")\n",
    "voice_model_file = os.path.join(project_root, \"src\", \"voice_instrumental-discogs-effnet-1.pb\")\n",
    "discogs_model_file = os.path.join(project_root, \"src\", \"discogs-effnet-bs64-1.pb\")\n",
    "musicnn_model_file = os.path.join(project_root, \"src\", \"msd-musicnn-1.pb\")\n",
    "danceability_model_file = os.path.join(project_root, \"src\", \"danceability-discogs-effnet-1.pb\")\n",
    "emotion_model_file = os.path.join(project_root, \"src\", \"emomusic-msd-musicnn-2.pb\")\n",
    "\n",
    "# Extract all features including embeddings, genre activations, voice/instrumental,\n",
    "# danceability (classifier mode), and emotion (arousal/valence).\n",
    "all_features = extract_all_features(\n",
    "    audio_dict, \n",
    "    tempo_method='tempocnn', \n",
    "    tempo_model_file=tempo_model_file,\n",
    "    emb_discogs_model_file=discogs_model_file,\n",
    "    emb_msd_model_file=musicnn_model_file,\n",
    "    genre_model_file=genre_model_file,\n",
    "    voice_model_file=voice_model_file,\n",
    "    danceability_model_file=danceability_model_file,\n",
    "    emotion_model_file=emotion_model_file\n",
    ")\n",
    "\n",
    "print(\"\\nAll Integrated Extracted Features (with embeddings, genre, voice, danceability, and emotion):\")\n",
    "pprint(all_features)\n",
    "\n",
    "# Optionally, display the results in a DataFrame.\n",
    "df = pd.DataFrame([all_features])\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate total extraction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the sample folder (e.g., one of the audio chunk folders).\n",
    "sample_folder = os.path.join(project_root, \"data\", \"raw\", \"audio_chunks\", \"audio.000\")\n",
    "\n",
    "# Recursively find all .mp3 files in that folder.\n",
    "sample_files = glob.glob(os.path.join(sample_folder, \"**\", \"*.mp3\"), recursive=True)\n",
    "# Filter out non-audio artifacts (e.g., files with \":\" in the name).\n",
    "sample_files = [f for f in sample_files if \":\" not in f]\n",
    "\n",
    "if not sample_files:\n",
    "    raise ValueError(\"No sample audio files were found in the folder: \" + sample_folder)\n",
    "\n",
    "# Optionally limit the sample size.\n",
    "sample_files = sample_files[:10]\n",
    "print(f\"Processing a sample of {len(sample_files)} files for time estimation...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Open a sample CSV file to write results.\n",
    "sample_csv = os.path.join(project_root, \"data\", \"processed\", \"sample_features.csv\")\n",
    "with open(sample_csv, 'w', newline='') as csvfile:\n",
    "    writer = None  # We'll create the DictWriter once we have a feature dictionary.\n",
    "    for file_path in tqdm(sample_files, desc=\"Processing sample files\"):\n",
    "        try:\n",
    "            # Load the audio and extract features.\n",
    "            audio_dict = load_audio_file(file_path, targetMonoSampleRate=44100, targetTempoSampleRate=11025)\n",
    "            features = extract_all_features(\n",
    "                audio_dict, \n",
    "                tempo_method='tempocnn', \n",
    "                tempo_model_file=tempo_model_file,\n",
    "                emb_discogs_model_file=discogs_model_file,\n",
    "                emb_msd_model_file=musicnn_model_file,\n",
    "                genre_model_file=genre_model_file,\n",
    "                voice_model_file=voice_model_file,\n",
    "                danceability_model_file=danceability_model_file,\n",
    "                emotion_model_file=emotion_model_file\n",
    "            )\n",
    "            # Include the file path for reference.\n",
    "            features['file'] = file_path\n",
    "            # Initialize the CSV writer on the first successful extraction.\n",
    "            if writer is None:\n",
    "                fieldnames = list(features.keys())\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "            writer.writerow(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            # Optionally log the traceback or continue.\n",
    "end_time = time.time()\n",
    "sample_duration = end_time - start_time\n",
    "avg_time_per_file = sample_duration / len(sample_files)\n",
    "\n",
    "# Count all audio files in the entire raw directory.\n",
    "all_files = []\n",
    "for root, _, files in os.walk(os.path.join(project_root, \"data\", \"raw\")):\n",
    "    for file in files:\n",
    "        if file.lower().endswith((\".mp3\", \".wav\", \".flac\", \".ogg\", \".m4a\")) and \":\" not in file:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "total_files = len(all_files)\n",
    "estimated_total_time = avg_time_per_file * total_files\n",
    "\n",
    "print(f\"\\nSample processing time: {sample_duration:.2f} seconds for {len(sample_files)} files\")\n",
    "print(f\"Average time per file: {avg_time_per_file:.2f} seconds\")\n",
    "print(f\"Estimated total processing time for {total_files} files: {estimated_total_time:.2f} seconds (~{estimated_total_time/60:.2f} minutes)\")\n",
    "print(f\"Sample features saved to: {sample_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell: Run the checkpoint-based extraction\n",
    "\n",
    "from src.audio_analysis2 import process_all_audio_with_checkpoint\n",
    "\n",
    "# Define the raw data directory and the checkpoint (features) directory.\n",
    "raw_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "checkpoint_dir = os.path.join(project_root, \"data\", \"processed\", \"features\")\n",
    "\n",
    "# Define model file paths.\n",
    "tempo_model_file = os.path.join(project_root, \"src\", \"deeptemp-k16-3.pb\")\n",
    "emb_discogs_model_file = os.path.join(project_root, \"src\", \"discogs-effnet-bs64-1.pb\")\n",
    "emb_msd_model_file = os.path.join(project_root, \"src\", \"msd-musicnn-1.pb\")\n",
    "genre_model_file = os.path.join(project_root, \"src\", \"genre_discogs400-discogs-effnet-1.pb\")\n",
    "voice_model_file = os.path.join(project_root, \"src\", \"voice_instrumental-discogs-effnet-1.pb\")\n",
    "danceability_model_file = os.path.join(project_root, \"src\", \"danceability-discogs-effnet-1.pb\")\n",
    "emotion_model_file = os.path.join(project_root, \"src\", \"emomusic-msd-musicnn-2.pb\")\n",
    "\n",
    "# Run the extraction with checkpointing.\n",
    "process_all_audio_with_checkpoint(\n",
    "    raw_dir,\n",
    "    checkpoint_dir,\n",
    "    tempo_method=\"tempocnn\",\n",
    "    tempo_model_file=tempo_model_file,\n",
    "    emb_discogs_model_file=emb_discogs_model_file,\n",
    "    emb_msd_model_file=emb_msd_model_file,\n",
    "    genre_model_file=genre_model_file,\n",
    "    voice_model_file=voice_model_file,\n",
    "    danceability_model_file=danceability_model_file,\n",
    "    emotion_model_file=emotion_model_file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amplab_essentia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
