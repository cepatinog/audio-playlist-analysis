# Final Report on Audio Analysis & Playlist Generation:

This report summarizes the design decisions, methodologies, observations, and personal reflections I developed over the course of the project. All source code, extracted features, statistical analyses, and UI implementations are included in the project repository for further review:

## ğŸš€ GitHub Repository
ğŸ”— **[View the complete source code and resources on GitHub](https://github.com/cepatinog/audio-playlist-analysis.git)** ğŸ”—

## 1. Overview
In this project, I developed a modular system to automatically extract meaningful musical features from a large audio collection, perform statistical analyses, and build interactive user interfaces for playlist generation. The system consists of three major components:

- **Feature Extraction:** I implemented a pipeline that uses Essentia and several pre-trained TensorFlow models to extract descriptors (tempo, key/scale, loudness, emotion, voice/instrumental classification, danceability, and music style activations) along with embeddings from two distinct models (Discogsâ€‘Effnet and MSDâ€‘MusicCNN).
- **Statistical Overview:** Extracted features were aggregated into final descriptor files and analyzed using Jupyter notebooks. Visualizations (histograms, scatter plots, and count plots) were generated to evaluate the distribution of tempo, loudness, key/scale, emotion, and genre.
- **Playlist Generation Interfaces:** I built two Streamlit apps:
  - A descriptorâ€‘based UI that filters tracks by tempo, key, danceability, emotion, and genre.
  - A similarityâ€‘based UI that computes cosine similarity between track embeddings (both Discogsâ€‘Effnet and MSDâ€‘MusicCNN) to create playlists of similar tracks.

---

## 2. Feature Extraction and Descriptor Calculation

### 2.1 Extraction Pipeline
My extraction pipeline is defined in `audio_analysis.py` and several dedicated modules (e.g., `extract_tempo.py`, `extract_key.py`, etc.). Key decisions include:

- **Tempo:** I opted for the TempoCNN method (using the `deeptemp-k16-3.pb` model) to provide a global BPM estimate and frameâ€‘wise local estimates with confidence scores. This choice offers robust tempo information even in complex musical passages.

- **Key and Scale:** I computed key estimates using three profilesâ€”Temperley, Krumhansl, and EDMAâ€”to capture different tonal perceptions. For further processing, I chose to use the Temperley profile because its estimates (key, scale, and strength) were consistent and aligned well with my subjective listening tests (with ~53% agreement across profiles).

- **Loudness:** Using Essentiaâ€™s `LoudnessEBUR128` algorithm, I extracted momentary, shortâ€‘term, and integrated loudness (in LUFS) as well as loudness range. These measures allowed me to compare overall dynamics and track loudness consistency.

- **Genre and Embeddings:**
  - **Embeddings:** I extracted two separate embedding vectors: a 1280â€‘dimensional vector using the Discogsâ€‘Effnet model (trained on 400 music styles) and a 200â€‘dimensional vector from MSDâ€‘MusicCNN (trained on 50 music tags).
  - **Genre Predictions:** From the 400â€‘dimensional activations provided by Discogsâ€‘Effnet, I predicted the style by taking the class with the highest activation. The full style label (e.g., â€œElectronic---Technoâ€) was then split to extract the parent genre for a higherâ€‘level view.

- **Voice/Instrumental & Danceability:**
  - The voice/instrumental classifier outputs a softmax probability, and I assign a binary label (â€œvoiceâ€ or â€œinstrumentalâ€) based on the maximum probability.
  - Danceability is estimated in two ways: a signalâ€‘based measure (using Essentiaâ€™s algorithm, yielding a value roughly in [0, 3]) and a classifierâ€‘based measure (using the Discogsâ€‘Effnet embedding as input to a danceability classifier, outputting a probability in [0, 1]).

- **Emotion (Arousal/Valence):** I computed emotion descriptors using a regression model (`emomusicâ€‘msdâ€‘musicnnâ€‘2`) that operates on an averaged MSDâ€‘MusicCNN embedding. The predictions for arousal and valence are expected to fall within [1, 9].

### 2.2 Descriptor Files
Final descriptors (excluding raw embeddings) were saved into CSV and TSV files inside the `apps/playlist_data` folder. These files include fields such as file path, tempo, voice/instrumental label, danceability values, emotion (arousal and valence), key, scale, predicted genre, and parent genre.

Separately, I stored the two sets of embeddings (Discogsâ€‘Effnet and MSDâ€‘MusicCNN) in CSV/TSV files so that similarityâ€‘based processing can operate independently.

---

## 3. Statistical Analysis & UI for Playlist Generation

### 3.1 Statistical Overview
Using Jupyter notebooks (e.g., `analysis_overview.ipynb`), I loaded the final descriptors and generated several plots:

- **Tempo & Loudness:** Histograms of global BPM and integrated loudness provided insights into the rhythmic and dynamic characteristics of the collection.
- **Key and Scale:** Count plots for key and scale across the three estimation profiles highlighted both consensus and variability in tonal analysis.
- **Emotion:** A scatter plot in the arousal/valence space revealed clusters of tracks with similar emotional characteristics.
- **Genre Distribution:** Due to the large number of specific styles (400), I aggregated genres by their parent category (e.g., â€œElectronicâ€, â€œHip Hopâ€) to obtain a clearer picture of the collectionâ€™s style diversity.

### 3.2 User Interfaces
I implemented two Streamlit apps:

1. **Descriptor-Based UI (`descriptor_based_ui.py`):**
   - This interface lets users filter tracks by descriptors such as tempo range, danceability (both signalâ€‘based and classifierâ€‘based), emotion (arousal/valence), key, scale, and parent genre.
   - The app then displays a filtered list of tracks, generates an M3U8 playlist, and embeds audio previews.

2. **Similarity-Based UI (`similarity_based_ui.py`):**
   - In this app, users select a query track from the collection. 
   - The app computes cosine similarity between the queryâ€™s embeddings and those of all other tracks, using both the Discogsâ€‘Effnet and MSDâ€‘MusicCNN embeddings.
   - Two lists of the top 10 most similar tracks are producedâ€”each accompanied by embedded audio previewsâ€”allowing users to listen and compare the results.

---

## 4. Observations & Reflections

### 4.1 Feature Quality and System Performance
- **Robust Features:**  
  The extraction modules performed reliably on most tracks. For example, clear rhythmic tracks consistently returned global BPM estimates (e.g., 128 BPM) that matched my auditory impressions. The loudness extraction produced realistic LUFS values, and emotion predictions for dynamic tracks were within the expected range.

- **Challenges:**  
  - **Ambiguous Content:** Some tracks with ambiguous tonality or hybrid instrumentation occasionally resulted in less stable key estimates or conflicting voice/instrumental predictions.  
  - **Danceability Differences:** I observed that signalâ€‘based danceability values sometimes diverged from classifier outputs on borderline cases, suggesting that combining both methods might yield a more robust measure.

- **Embedding Similarity:**  
  In my similarity tests, the Discogsâ€‘Effnet embeddingsâ€”which are trained on a comprehensive set of 400 stylesâ€”often captured stylistic similarities more effectively. For instance, tracks with subtle electronic textures and specific rhythmic patterns were more reliably clustered together using Discogsâ€‘Effnet embeddings. By contrast, MSDâ€‘MusicCNN embeddings, while valuable in capturing tag-related nuances (e.g., instrumental versus vocal cues), sometimes missed finer stylistic details.

### 4.2 Playlist Generation via Similarity
The similarityâ€‘based UI provided two distinct playlists for each query track:

- **Discogs-Based Playlist:**  
  This playlist generally produced more coherent stylistic groupings. My listening tests indicated that tracks retrieved using Discogsâ€‘Effnet embeddings shared similar instrumentation, production aesthetics, and overall vibe.

- **MSD-Based Playlist:**  
  While the MSDâ€‘MusicCNN-based playlist was useful for exploring tagâ€related attributes, it occasionally returned tracks that, despite similar tag activations, sounded less similar in terms of overall style.

Based on my tests, the Discogsâ€‘Effnet embeddings appear to deliver superior performance in capturing perceptual similarity for playlist generation. I hypothesize that this is due to the modelâ€™s training on a diverse and detailed taxonomy of 400 music styles, which forces it to learn nuanced distinctions between genres.

---

## 5. Conclusion
In summary, this project demonstrates a robust system for extracting musical features, statistically analyzing a large audio collection, and interactively generating playlists based on both descriptor queries and track similarity. The modular design allowed me to combine multiple stateâ€‘ofâ€‘theâ€‘art models to provide a rich set of descriptors and embeddings.

### Personal Reflections
- The integration of diverse features (tempo, key, loudness, emotion, and style) has proven invaluable for understanding the collectionâ€™s musical landscape.
- Although certain extraction modules (e.g., key or voice/instrumental classification) sometimes produced ambiguous results on borderline tracks, the overall performance is strong.
- My listening tests indicate that the Discogsâ€‘Effnet embeddings most effectively capture musical similarity, likely due to their fineâ€‘grained training on a large and varied set of styles.


